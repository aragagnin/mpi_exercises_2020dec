# MPI easy exercises

## Exercise 1: fix deadlock

 - see `ese1_fix_deadlock.c`

## Exercise 2: change `mpi_pi.c` send/recv types

- see `ese_2_mpi_pi_rsend.c` for the `MPI_RSend` test

- see `ese_2_mpi_pi_bsend.c` for the `MPI_BSend` test

- see `ese_2_mpi_pi_ssend.c` for the `MPI_SSend` test

- see `ese_2_mpi_pi_send.c` for the `MPI_Send` test

## Exercise 3: write `mpi_pi.c` with MPI reduce operation

- see `mpi_pi_sum.c`

## Exercise 4: write `mpi_sum.c`

- Simplest case: we sum N numbers with k MPI ranks so that `N%k==0`. See  `ese3_mpi_sum_nranks_divides_N.c` execute with `mpirun -np 4 a.out 4000`

- Generic case: we sum N numbers with any k MPI ranks so that `k<N`. See  `ese3_mpi_sum_any_N.c`

- Simple case with a "proto" domain decomposition: as the `Simplest case` and the array are generated by `irank==0` and spreeaded to all MPI ranks. See `ese3_mpi_sum_nranks_divides_N_domain_decomposition.c`

- generic case with a "proto" domain decomposition: as the `Generic case` and the array are generated by `irank==0` and spreeaded to all MPI ranks. See `ese3_mpi_sum_any_N_domain_decomposition.c`

## Exercise 5: topologies

- Implement a periodic 1D ring  and send data to the next MPI rank (thus the last will send to the first). See `ring.c`

- Implement a periodic 2D topology and use it to transpoe a matrix `ring2d.c`

## Exercise 6: mpi binding

- try `mpirun --map-by core`  with `print_affinity.c`

- try `mpi-run --map-by socket` with `print_affinity.c`

